|   Course Name    | Topic | Professor |      Date      | Tags |
| :--------------: | :---: | :-------: | :------------: | :--: |
| **Name in Bold** | Topic | Professor | Date of course |      |

[Class Video Link](URL)

# Summary
*A 3-4 sentence description of what was learned in italics*

# Key Takeaways
1. A histogram of the residuals is not an appropriate evaluation because the residuals are not identically distributed
2. The t-value is the ability, under Gaussian settings, to [[Hypothesis Testing|test]] if $b=0$ or $b\ne0$
3. $R^2$ is what is used when we are not operating in a Gaussian setting to see if there is a linear connection between the dependent and independent variables
4. $R^2$ can be defined as $1 -$ the ratio of the residual sum of squares to the total sum of squares $SSR/SST$ or as the sum of explained residuals over the sum of total residuals $SSE/SST$

# Definitions
- Regression t-value: The ratio between a parameter estimate and its standard error
- R-squared value: The ratio between the square [[Vectors|norm]] of the difference between the predicted and average values with the square norm of the difference between the actual values and the average
	- $\frac{||\hat Y-\bar Y_n||^2}{||Y-\bar Y_n||^2} = \frac{\sum(\hat Y_k - \bar Y_n)^2}{\sum(Y - \bar Y_n)^2} = 1 - \frac{\sum(Y_k - \hat Y_k)^2}{\sum(Y_k - \bar Y_n)^2}$
- Total Variance ($SST$): $||Y - \bar Y_n||^2$
- Residual Variance ($SSR$): $||Y-\hat Y||^2$
- Explained Variance ($SSE$): $||\hat Y - \bar Y_n||^2$
	- Variance of the $Y_k$

# Additional Resources
- Name the hyperlink in brackets then outside the brackets put the URL in parens

# Notes
## Section 1
- Bullets on the actual content in question